{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request,urlopen\n",
    "from googlesearch import search \n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    with open(filename) as f:\n",
    "        textfile = f.readlines()\n",
    "        texttemp = f.read()\n",
    "    sentences = []\n",
    "    for line in textfile:\n",
    "        line = line.strip('\\n')\n",
    "        for sent in line.split('.'):\n",
    "            sent = sent.strip()\n",
    "            sent = re.sub(r'[^\\w\\s]','',sent)\n",
    "            sent = re.sub(' +', ' ', sent)\n",
    "            sent = re.sub('[^a-zA-Z0-9 \\n\\.]', '', sent)\n",
    "            if len(sent)>1:\n",
    "                sentences.append(sent.lower())\n",
    "    return sentences\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './corpus-20090418/orig_taske.txt'\n",
    "test0  = preprocess(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = set()\n",
    "test_set.update(test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://git.homelabs.space/maxfan8/plagr/commit/44db18a31370d68056aafde09bc480a8492adfde.diff\n",
      "https://www.scribd.com/document/59636159/Dynamic-Programming-Wikip\n",
      "https://1library.net/document/zp0w56rq-hybrid-algorithm-optimization-problems-applied-single-machine-scheduling.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.dl.edi-info.ir/Unit%20commitment%20by%20dynamic%20programming%20for%20microgrid%20operational%20planning.pdf\n",
      "https://en.wikiversity.org/wiki/Dynamic_programming\n",
      "https://news.ycombinator.com/item?id=1060506\n",
      "http://mjclement.com/tests/js/tree_explorer/wikipedia/Dynamic_programming.html\n",
      "https://en.wikiversity.org/wiki/Dynamic_programming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.math.bas.bg/keleved/aubg/2020/COS_460_week07_slide.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mosaicprojects.com.au/Mag_Articles/P037_The_Origins_of_CPM.pdf\n",
      "https://www.wikiwand.com/en/Talk:Dynamic_programming/Archive_3\n",
      "https://gitlab.iguw.tuwien.ac.at/aurora/aurora/blob/16f766472ab1f6163f836f91034d15e22cbdcb3b/PlagCheck/test/data/sheffield/orig_taske.txt\n",
      "https://gitlab.iguw.tuwien.ac.at/aurora/aurora/blob/e1cd0ced7eade87f4e4344db8df77d45ff570a41/PlagCheck/test/data/sheffield/g4pC_taske.txt\n",
      "https://slideplayer.com/slide/4824012/\n",
      "http://mjclement.com/tests/js/tree_explorer/wikipedia/Dynamic_programming.html\n",
      "https://gitlab.iguw.tuwien.ac.at/aurora/aurora/blob/a883aa4a31e9a9b7cd5555f1f2e7d264663010d5/PlagCheck/test/data/sheffield/g3pB_taske.txt\n",
      "http://emergentexecs.com/docs/4nfxz.php?fa091f=history-of-dynamic-programming\n",
      "https://en.wikiversity.org/wiki/Dynamic_programming\n",
      "https://www.sofiaochjohan.se/blog/xv8xuv.php?935b07=history-of-dynamic-programming\n",
      "http://academickids.com/encyclopedia/index.php/Dynamic_programming\n",
      "http://mjclement.com/tests/js/tree_explorer/wikipedia/Dynamic_programming.html\n",
      "https://www.planetcalypsoforum.com/forums/showthread.php?37166-Q-amp-A-Answers-(Aug-Sep-06)/page22&highlight=heavy+weapon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://gtt-technologies.de/wp-content/uploads/2008/06/Emler_2008_Abstract.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://hua-zhou.github.io/teaching/st790-2015spr/ST790-2015-Spring-Pre-LecNotes.pdf\n",
      "https://slideplayer.com/slide/17635866/\n",
      "https://gitlab.iguw.tuwien.ac.at/aurora/aurora/blob/16f766472ab1f6163f836f91034d15e22cbdcb3b/PlagCheck/test/data/sheffield/orig_taske.txt\n",
      "https://www.planetcalypsoforum.com/forums/showthread.php?37166-Q-amp-A-Answers-(Aug-Sep-06)/page22&highlight=heavy+weapon\n",
      "http://shpora.net/index.cgi?act=view&id=9371\n"
     ]
    }
   ],
   "source": [
    "copied = set()\n",
    "url_set = set()\n",
    "errcnt = 0\n",
    "stored_text_corpus = ''\n",
    "for item in test0:\n",
    "    if item in copied:\n",
    "        continue\n",
    "    if len(item.split())>3:\n",
    "        x = \"\\\"\" + item + \"\\\"\"\n",
    "        for j in search(x, tld=\"co.in\", num=3, stop=3, pause=7): \n",
    "            if 'youtube' not in j:\n",
    "                url_set.update([j])\n",
    "                url = j\n",
    "                try:\n",
    "                    html =requests.get(url).content\n",
    "                    temptext = text_from_html(html)\n",
    "                    temptext =  re.sub(r'[^\\w\\s]','',temptext.lower())\n",
    "                    temptext = re.sub(' +', ' ', temptext)\n",
    "                    temptext = re.sub('[^a-zA-Z0-9 \\n\\.]', '', temptext)\n",
    "                    text_corpus = temptext\n",
    "                    stored_text_corpus+= text_corpus\n",
    "                    for line in test_set:\n",
    "                        xx = re.search(line,stored_text_corpus)\n",
    "                        if xx !=None:\n",
    "                            copied.update([line])\n",
    "                            test_set.remove(line)\n",
    "                except:\n",
    "                    errcnt+=1\n",
    "                    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'also scrapy comes with a bunch of middlewares for cookies redirects sessions caching etc',\n",
       " 'beautifulsoup beautiful soup is a library for parsing html and xml documents',\n",
       " 'choosing the right tool depends on the type of project you are working on since python has a wide variety of libraries and frameworks for web scraping',\n",
       " 'for scraping simpler static lessjs related complexities then this tool is probably what youre looking for',\n",
       " 'however it is not as efficient as the tools which we have discussed so far',\n",
       " 'i believe knowing the pros and cons of a tool helps in choosing the best tool for your project which helps in doing effective planning that may save you hours upfront',\n",
       " 'if you want to know more about beautifulsoup please refer to my previous guide on extracting data from html with beautifulsoup',\n",
       " 'if you want to know more about scrapy please refer to my previous guide on crawling the web with python and scrapy',\n",
       " 'if you want to know more about selenium please refer to web scraping with selenium',\n",
       " 'in scrapy we create spiders which are python classes that define how a particular sitesites will be scrapped',\n",
       " 'lxml is a highperformance straightforward fast and featurerich parsing library which is another prominent alternative to beautifulsoup',\n",
       " 'python is known for its famous and popular libraries and frameworks in web scraping',\n",
       " 'requests handles http sessions and makes http requests in combination with beautifulsoup a parsing library are the best package tools for small and quick web scraping',\n",
       " 'scrapy scrapy is a web crawling framework that provides a complete tool for scraping',\n",
       " 'selenium for heavyjs rendered pages or very sophisticated websites selenium webdriver is the best tool to choose',\n",
       " 'selenium is a tool that automates the webbrowsers also known as a webdriver',\n",
       " 'so if you want to build a robust concurrent scalable large scale scraper then scrapy is an excellent choice for you',\n",
       " 'so it is your responsibility to choose the best one for your project',\n",
       " 'that helps you to deal with different complexities that you might come across',\n",
       " 'the three most popular tools for web scraping are',\n",
       " 'this tool is something to use when all doors of web scraping are being closed and you still want the data which matters to you',\n",
       " 'with this you can open a google chromemozilla firefox automated window which visits a url and navigates on the links'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal substructure means that optimal solutions of subproblems can be used to find the optimal solutions of the overall problem\n",
      "because both f3 and f4 are needed to compute f5 a naive approach to computing f5 may end up computing f2 twice or more\n"
     ]
    }
   ],
   "source": [
    "for item in test_set:\n",
    "    if len(item.split())>3:\n",
    "        if item not in copied:\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.10344827586206"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plagirism_percentage =100* len(copied)/(len(test_set)+len(copied))\n",
    "plagirism_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
